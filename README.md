# Pipeline de dados - Combust√≠veis Brasil

O objetivo desse projeto foi criar um pipeline de dados para extrair informa√ß√µes de uma base de dados p√∫blica acerca do pre√ßo dos combust√≠veis no Brasil. 

- **Fonte de Dados:** 
  - Site: https://dados.gov.br/dataset/serie-historica-de-precos-de-combustiveis-por-revenda
  - Metadados: https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos/shpc/metadados-serie-historica-precos-combustiveis.pdf

Para este projeto, foram usados as seguintes tecnologias: 

- **Airflow**: Para realizar a tarefa de orquestrar o pipeline;
- **Google Cloud Storage:** Para armazenamento de dados Brutos e Curados;
- **Big Query:** Para armazenamento de dados curados;
- **DataStudio:** Para cria√ß√£o do Dashboard;
- **Google Cloud Dataproc:** Para processamento de dados com Spark;
- **Google Cloud Run:** Para hospedar a API que realize a coleta dos dados dos Combust√≠veis;

Os componentes foram organizados na seguinte arquitetura: 

![alt text](./img/combustiveis_brasil.png)

Para o Airflow, foi utilizada uma imagem docker que est√° no diret√≥rio /airflow. Entretanto, para  subir o servi√ßo localmente basta executar o comando **make** a partir do diret√≥rio raiz. 

A imagem abaixo ilustra a DAG criada para a orquestra√ß√£o. Existem dois operadores Dummy no inicio e fim para fins de organiza√ß√£o. A task get_op √© a respons√°vel por realizar a chamada da API presente no Cloud Run e assim extrair os dados da fonte. Em seguida, a task create_cluster cria um cluster dataproc para realizar o processamento dos dados com Spark. A pr√≥xima task roda o job de processamento dentro do cluster rec√©m criado e por √∫ltimo √© feita a remo√ß√£o do cluster.

![alt text](./img/dag.png)

A API foi feita utilizando o framework FastAPI e est√° presente no diret√≥rio /api. Para realizar o deploy dessa aplica√ß√£o no Cloud Run basta executar o shell script presente na mesma pasta. Para que o deploy seja de fato realizado, √© preciso estar logado em sua conta na Google Cloud Platform atrav√©s da SDK. Caso ainda n√£o tenha instalado, basta acessar esse [link](https://cloud.google.com/sdk/docs/install-sdk).

O pipeline em PySpark seguiu os seguintes passos: 

A fun√ß√£o main recebe como par√¢metro:
- path_input: Caminho dos dados no GCS gerados pela API coletora. Ex: gs://bucket_name/file_name.
- path_output: Caminho de onde ser√° salvo os dados processados. Ex: gs://bucket_name_2/file_name.
- formato_file_save: Formato de arquivo a ser salvo no path_output. Ex: PARQUET.
- dataset: Dataset no BigQuery onde est√° a tabela.
- tabela_bq: Tabela do BigQuery que ser√° salvo os dados. Ex: dataset.tabela_exemplo

Quanto a transforma√ß√£o de dados, foram realizadas as seguintes etapas: 

1. Fa√ßa a leitura dos dados de acordo com o path_input informado
2. Realize o rename de colunas do arquivo, respeitando os padr√µes do BigQuery
3. Adicione uma coluna de Ano, baseado na coluna `Data da Coleta`
4. Adicione uma coluna de Semestre, baseado na coluna de `Data da Coleta`
5. Adicione uma coluna Filename. Tip: pyspark.sql.functions.input_file_name
6. Fa√ßa o parse dos dados lidos de acordo com a tabela no BigQuery
7. Escreva os dados no Bucket GCS, no caminho informado `path_output`
no formato especificado no atributo `formato_file_save`.
8. Escreva os dados no BigQuery de acordo com a tabela especificada no atributo `tabela_bq`.

Por √∫ltimo, foi desenvolvido um dashboard usando a ferramenta Data Studio, da GCP. Abaixo est√° o link do relat√≥rio bem como o link para acesso (Nessa parte do projeto pe√ßo desculpas pois data viz n√£o √© o meu forte mas estou em busca de melhora üôÉ ). 

[Link](https://datastudio.google.com/s/o8V6TJH3OQU) para o dashboard.

![alt text](./img/dashboard.png)